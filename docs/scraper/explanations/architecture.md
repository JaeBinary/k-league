# 스크래퍼 아키텍처

이 문서에서는 K리그와 J리그 스크래퍼의 내부 동작 원리와 설계 철학을 설명합니다.

## 개념 소개

축구 리그 스크래퍼는 공식 웹사이트에서 경기 데이터를 자동으로 수집하는 도구입니다. 두 스크래퍼는 각 웹사이트의 특성에 맞게 서로 다른 수집 전략을 사용합니다.

## 아키텍처 비교

| 특성 | K리그 스크래퍼 | J리그 스크래퍼 |
|-----|--------------|--------------|
| 웹사이트 유형 | 정적 HTML | 동적 JavaScript |
| 파싱 도구 | BeautifulSoup | Selenium + BeautifulSoup |
| 데이터 접근 방식 | 경기 ID 순차 접근 | 월별 경기 목록 탐색 |
| 처리 모드 | 순차 처리 | 순차/병렬 처리 |

## K리그 스크래퍼 아키텍처

K리그 공식 웹사이트는 정적 HTML로 구성되어 있어 단순한 HTTP 요청과 HTML 파싱으로 데이터를 수집할 수 있습니다.

### 데이터 수집 흐름

```
┌──────────────────┐
│  URL 생성 단계   │  Step 1: 경기 ID 기반 URL 생성
│  (ID 순회)       │         - 1번부터 N번까지 순차 생성
└────────┬─────────┘         - 시즌별 경기 수 기반
         │
         ↓
┌──────────────────┐
│  HTML 다운로드    │  Step 2: HTTP 요청 및 페이지 파싱
│  (HTTP GET)      │         - BeautifulSoup HTML 파싱
└────────┬─────────┘         - 404/타임아웃 처리
         │
         ↓
┌──────────────────┐
│  데이터 추출 단계 │  Step 3: CSS Selector 기반 추출
│  (파싱)          │         - 메타데이터 (리그, 라운드, 날짜)
└────────┬─────────┘         - 팀 정보 (이름, 순위)
         │                   - 경기장 정보 (관중, 날씨)
         ↓
┌──────────────────┐
│  데이터 정제 단계 │  Step 4: 데이터 타입 변환 및 검증
│  (변환)          │         - 날짜 포맷 정규화
└────────┬─────────┘         - 관중 수 정수 변환
         │                   - 순위 정보 추출
         ↓
┌──────────────────┐
│  API 통계 수집   │  Step 5: K리그 공식 API 호출
│  (API 호출)      │         - matchRecord.do: 슈팅, 파울, 점유율 등
└────────┬─────────┘         - possession.do: 시간대별 점유율
         │
         ↓
┌──────────────────┐
│  통합 데이터셋    │  Step 6: 최종 데이터 반환
│  (리스트 반환)   │         - Dict[str, Any] 리스트
└──────────────────┘         - HTML 데이터 + API 통계 병합
```

### 핵심 설계 원리

1. **경기 ID 기반 접근**: K리그는 시즌별로 경기에 1부터 N까지 순차 ID를 할당합니다. 이를 활용하여 단순 반복문으로 모든 경기에 접근합니다.

2. **시즌별 경기 수 매핑**: 팀 수 변동에 따라 시즌별 경기 수가 다르므로 미리 정의된 매핑 테이블을 사용합니다.

```python
SEASON_MATCH_COUNT = {
    ("K리그1", 2025): 228,  # 12팀 기준
    ("K리그2", 2025): 275,  # 2025년 팀 수 증가
    ("승강PO", 2025): 4,
}
```

3. **CSS Selector 파싱**: BeautifulSoup의 `select_one()`, `select()` 메서드로 일관된 구조의 데이터를 추출합니다.

4. **API 통계 수집**: HTML 파싱 후 공식 API를 통해 추가 통계 데이터를 수집합니다.

```python
# Step 1: HTML에서 기본 정보 추출
data = parse_game_info(soup, year, game_id)

# Step 2: API에서 통계 데이터 추가
stats = get_match_stats(year, meet_seq, game_id)
if stats:
    data.update(stats)  # 점유율, 슈팅, 파울 등 병합
```

API에서 수집되는 데이터:
- **matchRecord.do**: 점유율, 슈팅, 파울, 카드, 코너킥, 프리킥, 오프사이드
- **possession.do**: 15분 단위 시간대별 점유율 (전반/후반 각 3구간)

## J리그 스크래퍼 아키텍처

J리그 공식 웹사이트는 JavaScript로 동적 렌더링되어 Selenium WebDriver가 필요합니다.

### BeautifulSoup이 아닌 Selenium을 사용하는 이유

J리그 웹사이트는 **클라이언트 사이드 렌더링(CSR)** 방식을 사용합니다. 서버는 데이터가 없는 빈 HTML 껍데기만 전송하고, 브라우저의 JavaScript가 실행되면서 데이터를 채워 넣습니다.

#### '빈 집' 비유로 이해하기

```
requests + BeautifulSoup이 보는 것:
┌─────────────────────────────┐
│  🏠 빈 집 (HTML 뼈대)        │
│  ┌─────┐ ┌─────┐ ┌─────┐   │
│  │     │ │     │ │     │   │  ← 가구(데이터)가 없음
│  │ <td>│ │ <td>│ │ <td>│   │
│  │     │ │     │ │     │   │
│  └─────┘ └─────┘ └─────┘   │
└─────────────────────────────┘

브라우저(Selenium)가 보는 것:
┌─────────────────────────────┐
│  🏠 가구가 배치된 집          │
│  ┌─────┐ ┌─────┐ ┌─────┐   │
│  │관중수│ │ 날씨 │ │ 온도 │   │  ← JavaScript가 데이터를 채움
│  │34860│ │ 맑음 │ │ 18℃ │   │
│  │     │ │     │ │     │   │
│  └─────┘ └─────┘ └─────┘   │
└─────────────────────────────┘
```

1. **requests가 하는 일**: 서버에 페이지를 요청하면, 서버는 **가구(데이터)가 없는 빈 집(HTML 뼈대)**만 전송합니다. BeautifulSoup은 이 빈 집을 아무리 뒤져봐도 데이터를 찾을 수 없습니다.

2. **브라우저가 하는 일**: 브라우저는 빈 HTML을 받자마자 **JavaScript를 실행**하여 서버 API에서 데이터를 가져와 `<td>` 태그 안에 채워 넣습니다.

3. **개발자 도구의 함정**: F12 개발자 도구에서 보이는 HTML은 JavaScript 실행 후의 모습입니다. 하지만 `Ctrl+U`(페이지 소스 보기)로 확인하면 데이터가 없는 빈 HTML만 보입니다.

#### 직접 확인하는 방법

```
1. J리그 경기 페이지에 접속
2. Ctrl + U (페이지 소스 보기) 실행
3. Ctrl + F로 "スタジアム" 또는 관중 수 검색
   → 검색되지 않음 (빈 HTML이기 때문)
```

이것이 바로 requests + BeautifulSoup이 보는 세상입니다. 데이터가 없으니 수집할 수 없습니다.

#### 해결책: Selenium

Selenium은 실제 Chrome 브라우저를 실행하여 JavaScript가 데이터를 모두 채울 때까지 기다린 후 HTML을 가져옵니다. 이 방식이 가장 확실하고 안정적입니다.

### 데이터 수집 흐름

```
┌──────────────────┐
│  URL 수집 단계   │  Step 1: 월별 경기 목록 URL 수집
│  (월별 순회)     │         - 1~12월 경기 리스트 크롤링
└────────┬─────────┘         - 경기 상세 페이지 URL 추출
         │
         ↓
┌──────────────────┐
│  데이터 추출 단계 │  Step 2: 개별 경기 페이지 스크래핑
│  (경기별 순회)   │         - 메타데이터 (라운드, 날짜, 팀명)
└────────┬─────────┘         - 경기장 정보 (관중, 날씨)
         │                   - 트래킹 데이터 (주행거리, 스프린트)
         ↓
┌──────────────────┐
│  데이터 정제 단계 │  Step 3: 데이터 타입 변환 및 검증
│  (파싱 및 변환)  │         - 문자열 → 숫자 변환
└────────┬─────────┘         - 날씨 정보 한글 번역
         │                   - 날짜 포맷 정규화
         ↓
┌──────────────────┐
│  통합 데이터셋    │  Step 4: 최종 데이터 반환
│  (리스트 반환)   │         - Dict[str, Any] 리스트
└──────────────────┘
```

### 핵심 설계 원리

1. **2단계 수집 전략**: 먼저 월별 페이지에서 모든 경기 URL을 수집한 후, 각 URL에서 상세 데이터를 추출합니다. 이를 통해 총 경기 수를 미리 파악하고 진행률을 표시합니다.

2. **Selenium WebDriver**: JavaScript 렌더링이 필요한 동적 페이지를 처리합니다. 탭 클릭, 요소 대기 등의 상호작용을 수행합니다.

3. **병렬 처리 지원**: ThreadPoolExecutor를 사용하여 여러 경기를 동시에 수집합니다. 각 스레드는 독립적인 WebDriver 인스턴스를 사용합니다.

4. **실패 경기 재수집**: 병렬 처리 중 실패한 경기는 별도로 모아서 순차 재시도합니다.

### 병렬 처리 구조

```
┌─────────────────────────────────────────────────┐
│              메인 스레드                         │
│  ┌─────────────────────────────────────────┐   │
│  │ [1단계] URL 수집                         │   │
│  │  - 단일 WebDriver로 1~12월 순회          │   │
│  │  - 모든 경기 URL 리스트 생성             │   │
│  └─────────────────────────────────────────┘   │
└──────────────────────┬──────────────────────────┘
                       │
                       ↓
┌─────────────────────────────────────────────────┐
│           ThreadPoolExecutor                     │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────┐│
│  │ Worker 1 │ │ Worker 2 │ │ Worker 3 │ │ ... ││
│  │(WebDriver)│ │(WebDriver)│ │(WebDriver)│ │     ││
│  │  경기 1   │ │  경기 2   │ │  경기 3   │ │     ││
│  │  경기 5   │ │  경기 6   │ │  경기 7   │ │     ││
│  │   ...    │ │   ...    │ │   ...    │ │     ││
│  └──────────┘ └──────────┘ └──────────┘ └──────┘│
└─────────────────────────────────────────────────┘
```

## 에러 처리 전략

### K리그 스크래퍼

- **404 응답**: 경기 페이지가 없으면 건너뜀 (아직 진행되지 않은 경기)
- **타임아웃**: fetch_page 함수 내부에서 처리, None 반환
- **파싱 오류**: 개별 필드 추출 실패 시 None 값 유지

### J리그 스크래퍼

- **TimeoutException**: 페이지 로딩 시간 초과 시 경기 건너뜀
- **NoSuchElementException**: 요소 없음 시 기본값 사용
- **SessionNotCreatedException**: ChromeDriver 생성 실패 시 최대 3회 재시도
- **병렬 처리 실패**: 실패 경기 목록화 후 순차 재수집

## 성능 특성

### K리그 스크래퍼

- 순차 처리만 지원
- 경기당 약 0.5초 (HTTP 요청 + 파싱)
- 메모리 효율적 (스트리밍 방식)

### J리그 스크래퍼

| 모드 | 경기당 소요시간 | 메모리 사용량 | 안정성 |
|-----|---------------|--------------|--------|
| 순차 | 약 2초 | 낮음 | 높음 |
| 병렬 (4워커) | 약 0.5초 | 중간 | 중간 |
| 병렬 (8워커) | 약 0.25초 | 높음 | 낮음 |

## 관련 문서

- [데이터 스키마](./data-schema.md)
- [병렬 처리 설정하기](../how-tos/parallel-collection.md)
- [트러블슈팅](../troubleshooting.md)
